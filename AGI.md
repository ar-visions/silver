# AGI Vision

I believe **AGI is us**. AGI, as I see it, is **Aggregated Group Intelligence** — decentralized data aggregated from truly identified sources, backed by cryptographic certification.  

Collectively and cooperatively, we offer a superior model to proprietary, somewhat metered contract models; ones that cannot enumerate public IP usage properly. Alternatively, with AGI, we summarize fair usage across IP identities (G) *provably* — a more eco-profitable model in which the public owns collectively. Throughout, we will see this integrated to the level of authorship ability in all devices we use. It’s more free for the user, yet a far more profitable model for corporations to adopt when making products.

Rather than singular corporations defining and maintaining expensive proprietary models, **we collectively create the schema and distributed dataset.** When corporations adopt this and train with integrity, they profit more with less cost on development. This enables all organizations to focus on their great products while including all of us to annotate using the devices they create. One practical use case: us driving a car to help annotate self-driving.

This publicly committed approach is *more provable* for IP than closed systems, especially when we all write the very operating systems they run on. We ensure verifiable barriers between public and private data, with the contract dictated by a free society.

**Being open-data is the only way IP law can work in an AI age.** Nobody can prove weights-only, private-corporate models in court. The training must be open for proof, with identifiable sources for IP origin.

---

## AI-PC

The process of media composition will become easier, more intuitive over time — with desktop eye-tracking AIs we can converse with for “public” screen elements. You use both your hands, eyes, and voice as cursors integrated in the machine. The next HID (Human Interface Device) is, in fact, the human themselves — without any gear at all.

For immersive observer-feedback on screen, the application view may counter-distort its projection with an ability to observe from different angles — like a virtual hologram using a 2D parallax technique applied in shaders. This is the particular focus of  [trinity](https://github.com/ar-visions/trinity) toolkit ✝ leveraging model from [hyperspace](https://github.com/ar-visions/hyperspace).

The product for this is **Orbiter** — it lets you run web4 apps with an ability to edit the source if it’s available.  We define apps as either console, graphical with desktop-spatial used when there are IR cameras <sub>[1]</sub>

<sub>[1]: Logitech Brio supported on Linux with it's low emission pulse-IR</sub>

---

*— Kalen Novis White*  
[www.silver-lang.org](https://www.silver-lang.org)