
## AI-PC

The process of media composition will become easier, more intuitive over time — with desktop eye-tracking AIs we can converse with for “public” screen elements. We use our hands, eyes, and voice as cursors integrated in the machine. The next HID (Human Interface Device) is, in fact, the human themselves — without any gear at all.

For immersive observer-feedback on screen, the application view may counter-distort its projection with an ability to observe from different angles — like a virtual hologram using a 2D parallax technique applied in shaders. This is the particular focus of  [trinity](https://github.com/ar-visions/trinity) toolkit ✝ leveraging model from [hyperspace](https://github.com/ar-visions/hyperspace).

The product for this is **Orbiter** — it runs web4 apps with an ability to live-edit the source.  We define apps as either console, graphical with desktop-spatial used when there are IR cameras [1].  As such, source editing will be rendered in both console, and graphical modes.

<sub>[1]: Logitech Brio supported on Linux with it's low emission pulse-IR</sub>

---

*— Kalen Novis White*  
[www.silver-lang.org](https://www.silver-lang.org)